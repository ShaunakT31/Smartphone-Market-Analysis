{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106717cb-6c1b-4bcb-9ce1-c3fd3c4c8d42",
   "metadata": {},
   "source": [
    "## Smartphone Market Analysis (₹20,000+ Segment) — Data Collection\n",
    "\n",
    "This notebook documents the process of **scraping smartphone data from Flipkart** to construct a comprehensive dataset for analyzing India’s **upper-midrange smartphone market (₹20,000 and above)**.\n",
    "\n",
    "The collected dataset aims to capture essential competitive parameters, including:\n",
    "\n",
    "- Price  \n",
    "- Brand  \n",
    "- Model Name  \n",
    "- Ratings and Review Count  \n",
    "- RAM and Storage Configuration  \n",
    "- Camera Specifications  \n",
    "- Battery Capacity  \n",
    "- Processor / Chipset  \n",
    "- Display Refresh Rate  \n",
    "\n",
    "This stage is dedicated exclusively to **data collection**. Subsequent notebooks will address **data cleaning, structuring, and analytical exploration**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f3530-82a3-4ead-9a3a-7f4950c97421",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Import Required Libraries\n",
    "\n",
    "This step involves importing the necessary Python libraries for data collection.  \n",
    "**Selenium** (along with **webdriver-manager**) is used to dynamically load Flipkart product pages and extract relevant smartphone listings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869b52eb-80e5-4559-a5aa-cfdec5facb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7e247-d641-4b51-99ad-31872654e13c",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Define Scraping Strategy\n",
    "\n",
    "The scraping process targets smartphone listings on Flipkart that meet the following criteria:\n",
    "\n",
    "- 5G smartphones  \n",
    "- Price ≥ ₹20,000  \n",
    "- Sorted by **Popularity** (to prioritize currently relevant and widely sold models)\n",
    "\n",
    "Each **brand** will be scraped individually to ensure accuracy and consistency across datasets.  \n",
    "All brand-specific datasets will be **merged** in subsequent stages for unified analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798e596-dac1-4aa8-8112-6281a2c41085",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Identify HTML Structure & CSS Selectors\n",
    "\n",
    "Analysis of Flipkart’s smartphone listing pages revealed the following key HTML elements and CSS selectors necessary for data extraction:\n",
    "\n",
    "- **Product card container:** `div.tUxRFH`  \n",
    "- **Product name:** `div.KzDlHZ`  \n",
    "- **Product price:** `div.Nx9bqj._4b5DiR`  \n",
    "- **Product specifications:** `ul.G4BRas > li.J+igdf`\n",
    "\n",
    "These selectors will be used with **BeautifulSoup** to parse and extract the required information from each loaded page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40149fc2-c7c8-46b1-8049-937c3d87b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded with 20k+ Oppo phones\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "url = \"https://www.flipkart.com/search?q=oppo+5g+smartphone&sort=popularity&p%5B%5D=facets.price_range.from%3D20000&p%5B%5D=facets.price_range.to%3DMax\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.tUxRFH\"))\n",
    ")\n",
    "\n",
    "print(\"Page loaded with 20k+ Oppo phones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc88b42-3110-45f1-bc1e-7e2414a8c928",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Scrape All Pages (Pagination)\n",
    "\n",
    "After successfully scraping data from a single Flipkart results page, the next step involves automating pagination to capture the complete set of listings.  \n",
    "\n",
    "The approach includes iterating through sequential pages by incrementing the **`page=`** parameter in the URL.  \n",
    "Scraping continues until no additional product cards are detected, ensuring that all available smartphones within the defined criteria are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852c0c6e-3f10-43e4-b8e9-b1f882186c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "Scraping Page 2...\n",
      "Scraping Page 3...\n",
      "Scraping Page 4...\n",
      "Scraping Page 5...\n",
      "Scraping Page 6...\n",
      "Scraping Page 7...\n",
      "Scraping Page 8...\n",
      "No more products. Stopping.\n",
      "Total products scraped: 156\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "all_products = []\n",
    "\n",
    "# Base URL for Oppo smartphones ≥ 20k\n",
    "base_url = \"https://www.flipkart.com/search?q=oppo+5g+smartphone&sort=popularity&p%5B%5D=facets.price_range.from%3D20000&p%5B%5D=facets.price_range.to%3DMax&page={}\"\n",
    "# Loop through pages\n",
    "for page in range(1, 10):\n",
    "    print(f\"Scraping Page {page}...\")\n",
    "    driver.get(base_url.format(page))\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    cards = soup.select(\"div.tUxRFH\")\n",
    "\n",
    "    if len(cards) == 0:\n",
    "        print(\"No more products. Stopping.\")\n",
    "        break\n",
    "\n",
    "    for card in cards:\n",
    "        name_tag = card.select_one(\"div.KzDlHZ\")\n",
    "        price_tag = card.select_one(\"div.Nx9bqj._4b5DiR\")\n",
    "        specs = card.select(\"ul.G4BRas li.J\\\\+igdf\")\n",
    "\n",
    "        name = name_tag.text.strip() if name_tag else None\n",
    "        price = price_tag.text.strip() if price_tag else None\n",
    "        specs_list = [s.text.strip() for s in specs] if specs else []\n",
    "\n",
    "        all_products.append({\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"specs\": specs_list\n",
    "        })\n",
    "\n",
    "print(\"Total products scraped:\", len(all_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b65fa2-df2b-44c5-b97a-a4e4ef5753e9",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Data Cleaning\n",
    "\n",
    "The raw scraped data requires initial preprocessing before analysis.  \n",
    "Key cleaning steps include:\n",
    "\n",
    "- Converting **price strings** (e.g., `\"₹24,999\"`) into integer values (`24999`)  \n",
    "- Parsing and normalizing the **specifications list** into structured fields:  \n",
    "  - RAM (GB)  \n",
    "  - Storage (GB)  \n",
    "  - Display Size (inches)  \n",
    "  - Rear Camera (MP)  \n",
    "  - Battery Capacity (mAh)  \n",
    "- Preparing the cleaned dataset for **CSV export**, enabling consistent formatting and easier integration into subsequent processing notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a61cb84-6a56-428d-8181-08e78fbd7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Drop rows where price is None\n",
    "df = df[df['price'].notna()].copy()\n",
    "\n",
    "# Convert price string → integer\n",
    "df['price'] = (\n",
    "    df['price']\n",
    "    .str.replace('₹', '', regex=False)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c0a8ff-61fe-4fd3-a127-0df62cbeabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ram(spec_list):\n",
    "    text = \" \".join(spec_list)\n",
    "    match = re.search(r'(\\d+)\\s*GB RAM', text, re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_storage(spec_list):\n",
    "    text = \" \".join(spec_list)\n",
    "    match = re.search(r'(\\d+)\\s*GB ROM', text, re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_display(spec_list):\n",
    "    text = \" \".join(spec_list)\n",
    "    match = re.search(r'(\\d+\\.\\d+|\\d+)\\s*inch', text, re.IGNORECASE)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "def extract_battery(spec_list):\n",
    "    text = \" \".join(spec_list)\n",
    "    match = re.search(r'(\\d+)\\s*mAh', text, re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_camera(spec_list):\n",
    "    text = \" \".join(spec_list)\n",
    "    match = re.search(r'(\\d+)\\s*MP', text, re.IGNORECASE)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "df['ram_gb'] = df['specs'].apply(extract_ram)\n",
    "df['storage_gb'] = df['specs'].apply(extract_storage)\n",
    "df['display_inch'] = df['specs'].apply(extract_display)\n",
    "df['battery_mah'] = df['specs'].apply(extract_battery)\n",
    "df['camera_mp'] = df['specs'].apply(extract_camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d3a4-b611-412b-8fa7-8f170e2819de",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Export Clean Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555319f4-29dc-4e1b-bf8a-bbd40e248a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"oppo_20k_clean.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26404b2-f9c7-4847-9cdf-79573d6c3a2c",
   "metadata": {},
   "source": [
    "---\n",
    "### Backing up Oppo scraped data\n",
    "Save a clean copy of the Oppo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a739318a-2288-4764-8d41-eb3f724e1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (25.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ec7943-d62d-4cbd-bd42-e0b435d58689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (22.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd2bc04-1682-4e59-b48f-58e446568d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - data/raw/oppo_20k_raw.json\n",
      " - data/clean/oppo_20k_clean.csv\n",
      " - data/clean/oppo_20k_clean.parquet\n",
      "Current cleaned df shape: (156, 8)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/clean\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Save raw list (all_products came from scraping)\n",
    "import json\n",
    "with open(\"data/raw/oppo_20k_raw.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_products, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save cleaned DataFrame (df) to CSV and parquet\n",
    "df.to_csv(\"data/clean/oppo_20k_clean.csv\", index=False)\n",
    "df.to_parquet(\"data/clean/oppo_20k_clean.parquet\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - data/raw/oppo_20k_raw.json\")\n",
    "print(\" - data/clean/oppo_20k_clean.csv\")\n",
    "print(\" - data/clean/oppo_20k_clean.parquet\")\n",
    "print(\"Current cleaned df shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8013cb71-ab1c-4065-9365-4674459161cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9b0ab-d8a8-4c67-bf53-2c1bbe48e7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
